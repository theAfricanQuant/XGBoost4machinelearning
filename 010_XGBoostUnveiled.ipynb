{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216bd37a-3748-4ff4-8083-69d631678fb4",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">XGBoost Unveiled</h1>\n",
    "\n",
    "Alright, let’s dive into a charming journey with Extreme Gradient Boosting, or XGBoost. Picture this: we’ve been exploring the wonderful world of machine learning, journeying from the simplicity of decision trees all the way up to the robustness of gradient boosting. Now, let’s sprinkle some magic, shall we?\n",
    "\n",
    "In the first half of this notebook, we're going to peel back the layers of XGBoost, revealing the secret sauce that gives it its powerful punch in tree ensemble algorithms. We'll wade through the theory and find out what really makes XGBoost tick and stand out in the crowd.\n",
    "\n",
    "In the second act, we’ll roll up our sleeves and get our hands dirty by building XGBoost models within the frame of the Higgs Boson Kaggle Competition - the very stage where XGBoost took its first bow and dazzled the world.\n",
    "\n",
    "What are we going to dig into, you ask?\n",
    "- We’ll unveil the tricks under XGBoost's hood that make it a speed demon in the machine learning drag race.\n",
    "- We'll explore the clever ways XGBoost deals with the pesky issue of missing values - it’s got some neat tricks up its sleeve!\n",
    "- And we’ll dive deep (but keep it breezy) into the mathematical wizardry that powers XGBoost's regularized parameter selection.\n",
    "\n",
    "We’ll get friendly with model templates, crafting our own XGBoost classifiers and regressors, becoming wizards in our own right.\n",
    "\n",
    "And for our grand finale, we’ll teleport ourselves to the Large Hadron Collider, the stage where the Higgs boson made its debut. Here, we’ll play with data, make some predictions, and get cozy with the original XGBoost Python API.\n",
    "\n",
    "Ready to embark on this adventure together? Let’s dive in, explore, and demystify XGBoost in our own relaxed, yet straight-to-the-point way.\n",
    "\n",
    "# Designing XGBoost\n",
    "XGBoost takes gradient boosting to the next level. In this part, we'll spotlight those unique characteristics of XGBoost that set it apart from traditional gradient boosting and other tree ensemble techniques.\n",
    "\n",
    "\n",
    "## A Historical Glimpse\n",
    "\n",
    "In the era of big data, the race to find powerful machine learning algorithms for optimal predictions took off. Decision trees were accurate but didn’t generalize well, while ensemble methods, like bagging and boosting, showed more promise. One standout was gradient boosting, which inspired Tianqi Chen to create **XGBoost**, bringing built-in regularization and remarkable speed gains to the table. After gaining recognition in Kaggle competitions, Chen and Carlos Guestrin introduced XGBoost to the wider machine learning community in 2016. [Read the original paper for more](https://arxiv.org/pdf/1603.02754.pdf).\n",
    "\n",
    "## Diving into Design Features\n",
    "\n",
    "XGBoost, aptly named for pushing computational limits to the extreme, addresses the need for faster algorithms in big data contexts. While our main focus here is building XGBoost models, we’ll sneak a peek under its hood to pinpoint key enhancements like handling missing values and improving speed and accuracy, which make it an attractive choice in the ML toolkit.\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "No need to stress over null values; XGBoost has got it covered with a `missing` hyperparameter. It smartly scores different split options and picks the best one when faced with missing data.\n",
    "\n",
    "### Speeding Things Up\n",
    "\n",
    "Designed with speed in mind, XGBoost quickly builds models even when grappling with massive datasets. Its design features that give it a speed advantage include:\n",
    "\n",
    "- Approximate split-finding algorithm\n",
    "- Sparsity-aware split-finding\n",
    "- Parallel computing\n",
    "- Cache-aware access\n",
    "- Block compression and sharding\n",
    "\n",
    "### Accuracy Gains with Regularization\n",
    "\n",
    "XGBoost doesn’t just stop at the gradient boosting; it includes built-in regularization to prevent overfitting and enhance accuracy, setting it apart from gradient boosting and random forests.\n",
    "\n",
    "# Crafting XGBoost Templates Together\n",
    "\n",
    "Let’s roll up our sleeves and create some handy templates for building XGBoost models! These will be your trusty guides, helping you craft XGBoost classifiers and regressors in your future adventures.\n",
    "\n",
    "## Classic Datasets as Our Playground\n",
    "\n",
    "We'll tinker with two classic datasets: the Iris for classification and the Diabetes for regression. Both are petite, nestled within scikit-learn, and well-explored by our fellow data explorers, providing us a common ground in the machine learning realm. \n",
    "\n",
    "And hey, we’ll get acquainted with some default hyperparameters along the way - they usually give XGBoost a good starting point, and knowing them will gear you up for any tuning adventures ahead!\n",
    "\n",
    "### The Iris Dataset: A Friendly Classic\n",
    "\n",
    "The Iris dataset, introduced by our friend Robert Fischer back in 1936, has been a darling of the machine learning community, thanks to its easy access, neat data, and symmetrical values. It's like the friendly neighborhood park where we all test our classification algorithms.\n",
    "\n",
    "Here’s how we invite the Iris dataset into our sandbox, straight from scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a16d8f2-e3fe-47d4-a70c-b581c68bf0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5759aaa4-4ce6-483f-a924-89cac7c0297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e79d7-2b72-4d42-95a6-1aaf7f572891",
   "metadata": {},
   "source": [
    "Scikit-learn tucks datasets into **NumPy arrays**, a beloved storage format for our machine learning escapades. Whereas, **pandas DataFrames** tend to be the champions for diving into data analysis and crafting visualizations. To peek at NumPy arrays through the lens of DataFrames, we enlist the `pandas DataFrame` method. Notably, scikit-learn datasets come pre-partitioned into predictor and target columns. To weave them back together, we concatenate the NumPy arrays with a dash of `np.c_` before making the conversion. np.c_ is a convenient attribute in NumPy for concatenating arrays along the second axis (i.e., columns). It's often used for horizontally stacking arrays (i.e., column-wise stacking), translating slice objects to concatenation along the second axis:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example arrays\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Using np.c_ for concatenation along the second axis\n",
    "c = np.c_[a, b]\n",
    "\n",
    "# Output\n",
    "# array([[1, 4],\n",
    "#        [2, 5],\n",
    "#        [3, 6]])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd17bdc-8a5d-4580-a5d7-51d958eaa9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b1f708-c791-4c77-9b00-b0143b7330f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "30                 4.8               3.1                1.6               0.2   \n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "138                6.0               3.0                4.8               1.8   \n",
       "67                 5.8               2.7                4.1               1.0   \n",
       "105                7.6               3.0                6.6               2.1   \n",
       "\n",
       "     target  \n",
       "30      0.0  \n",
       "0       0.0  \n",
       "138     2.0  \n",
       "67      1.0  \n",
       "105     2.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=np.c_[iris['data'],\n",
    "    iris['target']],\n",
    "    columns = iris['feature_names'] + ['target']\n",
    "            )\n",
    "\n",
    "df.sample(n=5, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4dac0-bae4-45bb-b90e-92c121b9e1af",
   "metadata": {},
   "source": [
    "The predictor columns, capturing sepal and petal dimensions, are straightforward. The target column encompasses three iris flower types: setosa, versicolor, and virginica, as outlined in the scikit-learn documentation, with a total of 150 entries. \n",
    "\n",
    "For machine learning prep, import `train_test_split` and partition the data. We will utilize the original NumPy arrays, `iris['data']` and `iris['target']`, as inputs for the splitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d37aa15-87f1-449a-a69e-4ef634cd108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], \n",
    "                                                    iris['target'], \n",
    "                                                    random_state=43\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61716d7-29f0-4758-832e-e496c3a570a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(booster='gbtree', \n",
    "                    objective='multi:softprob', \n",
    "                    max_depth=6, learning_rate=0.1, \n",
    "                    n_estimators=100, \n",
    "                    random_state=43, n_jobs=-1\n",
    "                   )\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81f2b1-367b-4c93-b5aa-1eafa50e7a8c",
   "metadata": {},
   "source": [
    "Let's delve into a brief exploration of the selected hyperparameters:\n",
    "\n",
    "##### a) Booster Type: `booster='gbtree'`\n",
    "   - **What is it?** The `booster` is the model (or \"learner\") that gets adjusted during the boosting rounds.\n",
    "   - **What does 'gbtree' mean?** It stands for gradient boosted tree, which is the default base learner in XGBoost.\n",
    "   - **Note**: While 'gbtree' is commonly used, we’ll explore other base learners in Chapter 8.\n",
    "\n",
    "##### b) Objective Function: `objective='multi:softprob'`\n",
    "   - **What is it?** The `objective` determines the loss function to be used in the model.\n",
    "   - **Why 'multi:softprob'?** It’s suitable for multiclass problems and outputs the predicted probability of each class. The class with the highest probability becomes the final prediction.\n",
    "   - **Extra Info**: XGBoost can often pick an appropriate objective if it's not explicitly defined. Dive into other options in the [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html).\n",
    "\n",
    "##### c) Tree Depth: `max_depth=6`\n",
    "   - **What is it?** `max_depth` specifies the maximum depth of a tree.\n",
    "   - **Why is it important?** It controls the complexity of the model by limiting the number of branches in the trees. A key parameter to tweak to avoid overfitting or underfitting.\n",
    "   - **Note**: XGBoost defaults to 6; in contrast, random forests don’t specify a `max_depth` unless defined.\n",
    "\n",
    "##### d) Learning Rate: `learning_rate=0.1`\n",
    "   - **What is it?** Also known as `eta` in XGBoost, the `learning_rate` scales the contribution of each tree.\n",
    "   - **Why does it matter?** It's a tuning knob, reducing the step size during boosting and thus controlling overfitting.\n",
    "   - **In Depth**: We explored this concept thoroughly in Chapter 4.\n",
    "\n",
    "##### e) Number of Trees: `n_estimators=100`\n",
    "   - **What is it?** `n_estimators` dictates the number of boosting rounds, or in simpler terms, the number of trees added to the model.\n",
    "   - **What’s the impact?** More trees can model more complexity, but also might lead to overfitting. Balancing this with `learning_rate` can often yield more robust models.\n",
    "\n",
    "Understanding each hyperparameter and its impact on the model is crucial for fine-tuning and achieving better predictive performance with XGBoost!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7d8f2d9-385b-41de-9573-0926d7ca5a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f'Score: {str(score)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c399cbe-3959-4d10-907c-ab9aca060bf7",
   "metadata": {},
   "source": [
    "# The Diabetes dataset\n",
    "In this section, an XGBoost regressor template is provided using cross_val_score with scikit-learn's Diabetes dataset.\n",
    "\n",
    "Before building the template, import the predictor columns as X and the target columns as y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a51c8142-2add-4634-8b84-f99409a45292",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62499c55-9b66-46ec-9d1f-511b33a6c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: [63.011 59.705 64.538 63.706 64.588]\n",
      "RMSE mean: 63.109\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(booster='gbtree', \n",
    "                   objective='reg:squarederror', \n",
    "                   max_depth=6, learning_rate=0.1, \n",
    "                   n_estimators=100, random_state=43, n_jobs=-1\n",
    "                  )\n",
    "\n",
    "scores = cross_val_score(xgb, X, y, \n",
    "                         scoring='neg_mean_squared_error', cv=5\n",
    "                        )\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print(f'RMSE: {np.round(rmse, 3)}')\n",
    "\n",
    "print(f'RMSE mean: {rmse.mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369b35b-93da-44fe-969b-f9ecc6474b85",
   "metadata": {},
   "source": [
    "Converting the target column, y, into a pandas DataFrame with the .describe() method will give the quartiles and the general statistics of the predictor column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2a97fe2-a960-4baf-96b8-cd01e2d1f14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>442.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>152.133484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>77.093005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>140.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>211.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>346.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  442.000000\n",
       "mean   152.133484\n",
       "std     77.093005\n",
       "min     25.000000\n",
       "25%     87.000000\n",
       "50%    140.500000\n",
       "75%    211.500000\n",
       "max    346.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9916b-5b26-4182-a898-9ecfddee2605",
   "metadata": {},
   "source": [
    "A score of 63.109 is less than 1 standard deviation, a respectable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605bfe7-3720-46d5-85fe-c55548623e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
