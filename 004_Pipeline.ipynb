{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee60ef9-bf3a-4651-9861-7de54ddceeac",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Pipelines</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c0a38-9eec-4682-8e1f-528625747e81",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Scikit-learn pipelines streamline the machine learning workflow by organizing sequential steps, preventing data leakage, and enhancing code readability. They offer a unified interface for training, prediction, and hyperparameter tuning. By encapsulating the entire process, pipelines ensure consistent results, simplify cross-validation, and facilitate smoother model deployment, making them an essential tool for efficient and error-free machine learning development.\n",
    "\n",
    "Often times when we are tackling some stuff in machine learning and datas science, we would need to perform a sequence of different transformations on the input data, such as finding or generating a set of features before performing some sort of estimation on it. Pipelines can be used to encapsulate the transfomers and the predictors to simplify the process. In other words, we can sequentially apply a list of transformers and a final estimator.\n",
    "\n",
    "### Pipelines in Machine Learning\n",
    "\n",
    "Machine learning workflows often involve a series of steps - from data cleaning and preprocessing to model training and evaluation. Managing these steps can become complex, especially when you have to ensure that the right preprocessing is done on both training and test data. Pipelines provide a way to streamline this process, ensuring that data transformations and model training are handled consistently.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### 1. Transformers\n",
    "Transformers are primarily used to modify or filter the dataset. They implement two key methods:\n",
    "\n",
    "- `fit()`: It computes the necessary parameters needed to apply the transformation. For instance, when scaling features, `fit` would compute the mean and standard deviation of the feature.\n",
    "- `transform()`: It applies the transformation to the data. Using the earlier example, `transform` would scale the features based on the computed mean and standard deviation.\n",
    "\n",
    "Commonly used transformers in `scikit-learn` include:\n",
    "- **Scaler classes** like `StandardScaler`, `MinMaxScaler`, etc., for feature scaling.\n",
    "- **`OneHotEncoder`** for converting categorical variables into a one-hot encoded format.\n",
    "- **`SimpleImputer`** for handling missing values.\n",
    "- **`PolynomialFeatures`** for generating polynomial combinations of features, as mentioned in your text.\n",
    "\n",
    "#### 2. Estimators\n",
    "Estimators are algorithms that can learn from data. They also implement the `fit()` method, but in addition to that, they have a `predict()` method to make predictions on new data. Some estimators also have a `transform()` method, making them both transformers and estimators.\n",
    "\n",
    "Commonly used estimators in `scikit-learn` include:\n",
    "- Regression models like `LinearRegression`, `Ridge`, and `Lasso`.\n",
    "- Classification models like `LogisticRegression`, `SVM`, and `RandomForestClassifier`.\n",
    "\n",
    "### Preprocessing Steps in `scikit-learn`\n",
    "\n",
    "1. **Data Cleaning**: Before preprocessing, it's crucial to clean the data, which might involve:\n",
    "   - Removing duplicates.\n",
    "   - Handling missing values using transformers like `SimpleImputer`.\n",
    "   - Removing outliers or erroneous data.\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Generating new features that might help improve model performance.\n",
    "   - Using transformers like `PolynomialFeatures` to generate interaction terms.\n",
    "\n",
    "3. **Feature Scaling**: Bringing all features to a similar scale using:\n",
    "   - `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance.\n",
    "   - `MinMaxScaler`: Transforms features by scaling each feature to a given range, usually [0, 1].\n",
    "\n",
    "4. **Encoding Categorical Variables**: Converting categorical data into a format suitable for machine learning models:\n",
    "   - `OneHotEncoder`: Converts categorical variables into a one-hot encoded format.\n",
    "   - `OrdinalEncoder`: Converts categorical variables into an integer format.\n",
    "\n",
    "5. **Feature Selection**: Reducing the number of input features, which can help improve model performance and reduce overfitting. Techniques include:\n",
    "   - Recursive Feature Elimination (RFE).\n",
    "   - Using `SelectKBest` to choose the top 'k' features based on certain criteria.\n",
    "\n",
    "6. **Data Splitting**: Dividing the data into training and test sets using `train_test_split`.\n",
    "\n",
    "### Constructing Pipelines in `scikit-learn`\n",
    "Pipelines can be constructed using the `Pipeline` class from `scikit-learn`. The `Pipeline` class takes tuples of the transformer alias (any name you choose to call it) and actual transformer object, all arranged in the order we want the transformations to be made on the datasets. We next call `fit_transform` on the train data but when its the test data, we call `transform` on it.\n",
    "\n",
    "The main advantage of using pipelines is to ensure that the same preprocessing steps are applied to both training and test data, reducing the risk of data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "We are going to go through all we have gone through in the previous notebooks this time passing everything through a pipeline and creating a transformer too.\n",
    "\n",
    "## Bike Rentals\n",
    "\n",
    "We will now proceed to test out pipelines to our first attempt with regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8631ea9e-5241-404d-a35c-73b807cfa6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wrangle_bike_rentals import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBRegressor, XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880114d5-e466-473c-96b9-24437b8e76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/theAfricanQuant/XGBoost4machinelearning/main/data/bike_rentals.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25962db6-1ebf-4522-b0fe-78b8ee0f0e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season   yr  mnth  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01     1.0  0.0   1.0      0.0      6.0         0.0   \n",
       "1        2  2011-01-02     1.0  0.0   1.0      0.0      0.0         0.0   \n",
       "2        3  2011-01-03     1.0  0.0   1.0      0.0      1.0         1.0   \n",
       "3        4  2011-01-04     1.0  0.0   1.0      0.0      2.0         1.0   \n",
       "4        5  2011-01-05     1.0  0.0   1.0      0.0      3.0         1.0   \n",
       "\n",
       "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
       "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
       "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
       "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
       "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
       "\n",
       "    cnt  \n",
       "0   985  \n",
       "1   801  \n",
       "2  1349  \n",
       "3  1562  \n",
       "4  1600  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b658850-2c66-47ce-a96a-b4b3d112241f",
   "metadata": {},
   "source": [
    "We are next going to create our transformer that will apply our `prep_data` function that we created in notebook 001. But we will modify it abit. We were too fancy with the inputation. Here we will just allow sklearn to handle that for us. We will delete that part and rename it to `prep_bike_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659f26c3-6160-4c38-a26c-205988931bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_bike_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())),\n",
    "                    hum = (data['hum']\n",
    "                   .fillna(data.groupby('season')['hum']\n",
    "                           .transform('median'))),\n",
    "                    dteday = pd.to_datetime(data['dteday']),\n",
    "                    mnth = lambda x: x['dteday'].dt.month,\n",
    "                    yr = data['yr'].ffill()\n",
    "                   )\n",
    "            .drop(['dteday', 'casual','registered'], axis=1)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5230dbbf-2a74-4819-a009-0f915f05a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepDataTransformer(BaseEstimator,\n",
    "    TransformerMixin):\n",
    "    \"\"\"\n",
    "    This transformer takes a Pandas DataFrame containing our survey \n",
    "    data as input and returns a new version of the DataFrame. \n",
    "    \n",
    "    ----------\n",
    "    ycol : str, optional\n",
    "        The name of the column to be used as the target variable.\n",
    "        If not specified, the target variable will not be set.\n",
    "    Attributes\n",
    "    ----------\n",
    "    ycol : str\n",
    "        The name of the column to be used as the target variable.\n",
    "    \"\"\"\n",
    "    def __init__(self, ycol=None):\n",
    "        self.ycol = ycol\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return prep_bike_data(X)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dd5b1-89e8-4553-a685-702c0bdc6cb2",
   "metadata": {},
   "source": [
    "Next we create the pipeline using the `Pipeline` class from sklearn. We will pass in tuples of the transformer alias and the actual transformer object we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f763c49-ab32-41fd-ba6d-33e7196d779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [('tweak', PrepDataTransformer()),\n",
    "     ('imputer', SimpleImputer(strategy='median')),  # Imputing null values using mean\n",
    "     ('scaler', StandardScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acebb3-d16f-4b04-aa3f-e5cefba85222",
   "metadata": {},
   "source": [
    "Time to split our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "165a517e-ebe5-4ef7-bca0-4688e46dd5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitX_y(df, trgt):\n",
    "    features = [col for col in df.columns if col not in trgt]\n",
    "    return (df[features], df[trgt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394b38ae-6c23-4368-ada8-86b768f140de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target vector: (731,)\n",
      "shape of feature matrix: (731, 15)\n"
     ]
    }
   ],
   "source": [
    "bikes_X, bikes_y = splitX_y(df, 'cnt')\n",
    "\n",
    "print(f\"shape of target vector: {bikes_y.shape}\")\n",
    "print(f\"shape of feature matrix: {bikes_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b6f9f4-812c-4944-83c8-9f09df75fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_X_train, bikes_X_test, bikes_y_train, bikes_y_test = (model_selection\n",
    "                                    .train_test_split(bikes_X, bikes_y, \n",
    "                                                      test_size=.3, \n",
    "                                                      random_state=43,)\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba706ea-4689-4152-afe3-cb40020ad678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42851322,  1.41014134, -0.99804496, ...,  0.62710628,\n",
       "         1.80584285, -0.95596646],\n",
       "       [-0.42382335,  1.41014134, -0.99804496, ...,  0.63094142,\n",
       "         1.54909312, -0.57245613],\n",
       "       [-1.0100577 , -0.38906521, -0.99804496, ...,  1.11278731,\n",
       "         0.34996609,  0.06987143],\n",
       "       ...,\n",
       "       [-0.39099422,  1.41014134, -0.99804496, ...,  0.34000416,\n",
       "         0.14281996, -0.21207628],\n",
       "       [-0.49417147,  0.51053806, -0.99804496, ...,  0.80298336,\n",
       "         0.59504315, -0.65839276],\n",
       "       [-0.1893296 ,  1.41014134, -0.99804496, ..., -0.89546132,\n",
       "        -0.36192674,  1.40103935]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pipe.fit_transform(bikes_X_train, bikes_y_train)\n",
    "X_test = pipe.transform(bikes_X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79e1a8ec-40f0-481b-bd6b-cd47f1f1b6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 983.05\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, bikes_y_train)\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(bikes_y_test, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734fbea-e9d8-47ac-a9db-fcb1b08a171c",
   "metadata": {},
   "source": [
    "This is worst that the first one. Let us see what the `xgboost` will get us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36a82869-4764-45cb-80e9-8745b1d25edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 647.12\n"
     ]
    }
   ],
   "source": [
    "xgb_reg = XGBRegressor()\n",
    "xgb_reg.fit(X_train, bikes_y_train)\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(bikes_y_test, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ea32f-a528-4cdc-919a-d680abef154c",
   "metadata": {},
   "source": [
    "I love this one!\n",
    "\n",
    "Next is Cross Validation. Here we pass in the entire pipeline as the scores will be more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1230eb19-c4fc-4fbc-a3db-5b8947b1c35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [ 504.73  842.99 1142.28  728.66  639.54  970.19 1133.83 1252.64 1085.75\n",
      " 1432.39]\n",
      "RMSE mean: 973.3001612624137\n"
     ]
    }
   ],
   "source": [
    "lr_pipe = Pipeline(\n",
    "    [('tweak', PrepDataTransformer()),\n",
    "     ('imputer', SimpleImputer(strategy='median')),  # Imputing null values using mean\n",
    "     ('scaler', StandardScaler()),\n",
    "     ('linreg', LinearRegression())\n",
    "    ]\n",
    ")\n",
    "scores = cross_val_score(lr_pipe, bikes_X, bikes_y, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print(f'Reg rmse: {np.round(rmse, 2)}')\n",
    "\n",
    "print(f'RMSE mean: {rmse.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce476963-4953-4d24-a1b4-c74060051185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [ 675.05  696.86  555.72  690.63  862.77 1039.04 1008.01  846.13  875.39\n",
      " 1649.44]\n",
      "RMSE mean: 889.9043382575034\n"
     ]
    }
   ],
   "source": [
    "xg_pipe = Pipeline(\n",
    "    [('tweak', PrepDataTransformer()),\n",
    "     ('imputer', SimpleImputer(strategy='median')),  # Imputing null values using mean\n",
    "     ('scaler', StandardScaler()),\n",
    "     ('xgb', XGBRegressor())\n",
    "    ]\n",
    ")\n",
    "scores = cross_val_score(xg_pipe, bikes_X, bikes_y, scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "print(f'Reg rmse: {np.round(rmse, 2)}')\n",
    "\n",
    "print(f'RMSE mean: {rmse.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7ba5d-9043-41bc-b8fe-72838ed47398",
   "metadata": {},
   "source": [
    "## Census\n",
    "\n",
    "We now switch our attention to the census data we worked with in the last notebook while working on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "847a7089-281d-44cd-8fb7-5276abec5f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education-num   32561 non-null  int64 \n",
      " 4   marital-status  32561 non-null  object\n",
      " 5   occupation      32561 non-null  object\n",
      " 6   relationship    32561 non-null  object\n",
      " 7   race            32561 non-null  object\n",
      " 8   sex             32561 non-null  object\n",
      " 9   capital-gain    32561 non-null  int64 \n",
      " 10  capital-loss    32561 non-null  int64 \n",
      " 11  hours-per-week  32561 non-null  int64 \n",
      " 12  native-country  32561 non-null  object\n",
      " 13  income          32561 non-null  object\n",
      "dtypes: int64(6), object(8)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "url_census = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "\n",
    "def prep_census(url):\n",
    "    col_names = ['age', 'workclass', 'fnlwgt', \n",
    "                 'education', 'education-num', \n",
    "                 'marital-status', 'occupation',\n",
    "                  'relationship', 'race', 'sex', \n",
    "                 'capital-gain', 'capital-loss', \n",
    "                 'hours-per-week', 'native-country', \n",
    "                   'income']\n",
    "    return (pd\n",
    "            .read_csv(url, header=None)\n",
    "            .pipe(lambda x: x.rename(columns={i: name for i, name in enumerate(col_names)}))\n",
    "            .drop(['education'], axis=1)\n",
    "    )\n",
    "\n",
    "df_census = prep_census(url_census)\n",
    "df_census.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c318f92-bcd9-4a9c-af01-2e31014e5e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['workclass',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'sex',\n",
       " 'native-country',\n",
       " 'income']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_census.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "643e4bff-db78-4c2d-b24a-c25b8aca823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHETransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer that applies one-hot encoding to columns of type 'object' \n",
    "    in a Pandas DataFrame. The one-hot encoding process converts categorical \n",
    "    columns into a format that can be provided to machine learning algorithms \n",
    "    to improve predictions.\n",
    "\n",
    "    The transformer identifies columns with data type 'object' and uses the\n",
    "    OneHotEncoder from the category_encoders library to perform the encoding.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cols : list\n",
    "        List of column names in the DataFrame identified for one-hot encoding.\n",
    "    \n",
    "    encode : OneHotEncoder object\n",
    "        The encoder instance from category_encoders that performs the actual \n",
    "        one-hot encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cols = None\n",
    "        self.encode = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        self.encode = OneHotEncoder(cols=self.cols, use_cat_names=False)\n",
    "        self.encode.fit(X[self.cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_encoded = self.encode.transform(X[self.cols])\n",
    "        X = X.drop(columns=self.cols)\n",
    "        return pd.concat([X, X_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d6e50f5-1736-4d4c-89e3-318595cf6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_pipe = Pipeline(\n",
    "    [('ohe', OHETransformer())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66005027-871f-4dd7-82a8-26a2f883ec0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target vector: (32561,)\n",
      "shape of feature matrix: (32561, 13)\n"
     ]
    }
   ],
   "source": [
    "cen_X, cen_y = splitX_y(df_census, 'income')\n",
    "\n",
    "print(f\"shape of target vector: {cen_y.shape}\")\n",
    "print(f\"shape of feature matrix: {cen_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9ccdd53-72b0-49b2-97be-4315949b5e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_1</th>\n",
       "      <th>workclass_2</th>\n",
       "      <th>workclass_3</th>\n",
       "      <th>workclass_4</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_33</th>\n",
       "      <th>native-country_34</th>\n",
       "      <th>native-country_35</th>\n",
       "      <th>native-country_36</th>\n",
       "      <th>native-country_37</th>\n",
       "      <th>native-country_38</th>\n",
       "      <th>native-country_39</th>\n",
       "      <th>native-country_40</th>\n",
       "      <th>native-country_41</th>\n",
       "      <th>native-country_42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20717</th>\n",
       "      <td>24</td>\n",
       "      <td>32950</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>37</td>\n",
       "      <td>34996</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28940</th>\n",
       "      <td>46</td>\n",
       "      <td>189498</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1848</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28302</th>\n",
       "      <td>50</td>\n",
       "      <td>301583</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10929</th>\n",
       "      <td>46</td>\n",
       "      <td>224559</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26901</th>\n",
       "      <td>42</td>\n",
       "      <td>86185</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7985</th>\n",
       "      <td>33</td>\n",
       "      <td>142675</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18687</th>\n",
       "      <td>36</td>\n",
       "      <td>35429</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19776</th>\n",
       "      <td>20</td>\n",
       "      <td>493443</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14148</th>\n",
       "      <td>32</td>\n",
       "      <td>396745</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2415</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22792 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "20717   24   32950             13             0             0              35   \n",
       "11366   37   34996              9             0             0              40   \n",
       "28940   46  189498             13             0          1848              45   \n",
       "28302   50  301583              9             0             0              40   \n",
       "10929   46  224559              9             0             0              40   \n",
       "...    ...     ...            ...           ...           ...             ...   \n",
       "26901   42   86185             10             0             0              40   \n",
       "7985    33  142675             13             0             0              30   \n",
       "18687   36   35429             10             0             0              60   \n",
       "19776   20  493443              7             0             0              40   \n",
       "14148   32  396745             13             0          2415              48   \n",
       "\n",
       "       workclass_1  workclass_2  workclass_3  workclass_4  ...  \\\n",
       "20717            1            0            0            0  ...   \n",
       "11366            0            1            0            0  ...   \n",
       "28940            0            0            1            0  ...   \n",
       "28302            0            0            1            0  ...   \n",
       "10929            0            0            1            0  ...   \n",
       "...            ...          ...          ...          ...  ...   \n",
       "26901            0            0            1            0  ...   \n",
       "7985             0            0            1            0  ...   \n",
       "18687            0            0            1            0  ...   \n",
       "19776            0            0            1            0  ...   \n",
       "14148            0            0            1            0  ...   \n",
       "\n",
       "       native-country_33  native-country_34  native-country_35  \\\n",
       "20717                  0                  0                  0   \n",
       "11366                  0                  0                  0   \n",
       "28940                  0                  0                  0   \n",
       "28302                  0                  0                  0   \n",
       "10929                  0                  0                  0   \n",
       "...                  ...                ...                ...   \n",
       "26901                  0                  0                  0   \n",
       "7985                   0                  0                  0   \n",
       "18687                  0                  0                  0   \n",
       "19776                  0                  0                  0   \n",
       "14148                  0                  0                  0   \n",
       "\n",
       "       native-country_36  native-country_37  native-country_38  \\\n",
       "20717                  0                  0                  0   \n",
       "11366                  0                  0                  0   \n",
       "28940                  0                  0                  0   \n",
       "28302                  0                  0                  0   \n",
       "10929                  0                  0                  0   \n",
       "...                  ...                ...                ...   \n",
       "26901                  0                  0                  0   \n",
       "7985                   0                  0                  0   \n",
       "18687                  0                  0                  0   \n",
       "19776                  0                  0                  0   \n",
       "14148                  0                  0                  0   \n",
       "\n",
       "       native-country_39  native-country_40  native-country_41  \\\n",
       "20717                  0                  0                  0   \n",
       "11366                  0                  0                  0   \n",
       "28940                  0                  0                  0   \n",
       "28302                  0                  0                  0   \n",
       "10929                  0                  0                  0   \n",
       "...                  ...                ...                ...   \n",
       "26901                  0                  0                  0   \n",
       "7985                   0                  0                  0   \n",
       "18687                  0                  0                  0   \n",
       "19776                  0                  0                  0   \n",
       "14148                  0                  0                  0   \n",
       "\n",
       "       native-country_42  \n",
       "20717                  0  \n",
       "11366                  0  \n",
       "28940                  0  \n",
       "28302                  0  \n",
       "10929                  0  \n",
       "...                  ...  \n",
       "26901                  0  \n",
       "7985                   0  \n",
       "18687                  0  \n",
       "19776                  0  \n",
       "14148                  0  \n",
       "\n",
       "[22792 rows x 92 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cen_X_train, cen_X_test, cen_y_train, cen_y_test = (model_selection\n",
    "                                    .train_test_split(cen_X, cen_y, \n",
    "                                                      test_size=.3, \n",
    "                                                      random_state=43,)\n",
    "                                                        )\n",
    "\n",
    "X_train = census_pipe.fit_transform(cen_X_train)\n",
    "X_test = census_pipe.transform(cen_X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f59f347-c4cf-442f-b4ae-5ed88688b94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='saga')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='saga', max_iter=10000)\n",
    "logreg.fit(X_train, cen_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "634a507b-b5c2-4e21-9c5b-a64d15048649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20717     <=50K\n",
       "11366     <=50K\n",
       "28940      >50K\n",
       "28302     <=50K\n",
       "10929     <=50K\n",
       "          ...  \n",
       "26901     <=50K\n",
       "7985      <=50K\n",
       "18687     <=50K\n",
       "19776     <=50K\n",
       "14148      >50K\n",
       "Name: income, Length: 22792, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cen_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb695b2a-3ff1-4346-b2cc-e9f3f468bea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7958849421639881"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, cen_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331c0e2-3286-48cf-821f-315e36577fc6",
   "metadata": {},
   "source": [
    "This looks so good. We will next check out xgboost. One issue though is the fact that xgboost won't accept our labels as is. We will need to encode it using sklearns `LabelEncoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "368f2425-3963-4e87-a2ce-d973561959d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56a0f722-aeae-42b7-a1d5-c3c6a927ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.fit_transform(cen_y_train)\n",
    "y_test = le.transform(cen_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "000c5a37-eeee-47c0-8609-af672ebc54d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' >50K'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2057e06f-bf13-4549-8363-2204460f0207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8708158460436073"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc= XGBClassifier()\n",
    "xgbc.fit(X_train, y_train)\n",
    "xgbc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd9fb8-01d2-413c-a68f-d8857ed7d159",
   "metadata": {},
   "source": [
    "Our score is great compared with that of Logistic regression. Let us put everything thru a cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e523c072-1beb-4501-9bea-de20769e3858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [0.8  0.8  0.79 0.79 0.79 0.8  0.79 0.8  0.8  0.8 ]\n",
      "RMSE mean: 0.7952765410203237\n"
     ]
    }
   ],
   "source": [
    "logreg_pipe = Pipeline(\n",
    "    [('ohe', OHETransformer()),\n",
    "     ('logreg', LogisticRegression(solver='saga', max_iter=10000))\n",
    "    ]\n",
    ")\n",
    "scores = cross_val_score(logreg_pipe, cen_X, cen_y, scoring='accuracy', cv=10)\n",
    "\n",
    "\n",
    "print(f'Reg rmse: {np.round(scores, 2)}')\n",
    "\n",
    "print(f'RMSE mean: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f36df3f3-35f6-4d89-9dcc-7f241a28be1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg rmse: [0.85 0.86 0.87 0.85 0.86 0.86 0.86 0.87 0.86 0.86]\n",
      "RMSE mean: 0.8591876766654168\n"
     ]
    }
   ],
   "source": [
    "xgbc_pipe = Pipeline(\n",
    "    [('ohe', OHETransformer()),\n",
    "     ('xgbc', XGBClassifier(n_estimators=5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "le_y_test = le.transform(cen_y) # transforming the labels like we did above\n",
    "\n",
    "scores = cross_val_score(xgbc_pipe, cen_X, le_y_test, scoring='accuracy', cv=10)\n",
    "\n",
    "\n",
    "print(f'Reg rmse: {np.round(scores, 2)}')\n",
    "\n",
    "print(f'RMSE mean: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e75d74-01db-409c-80a1-36201c452c66",
   "metadata": {},
   "source": [
    "Before we round up this part of the walk-through, let us create a `helper_file.py` file where we keep all the relevant helper functions we have worked on thus far. I know we had done that for the rental bike before, this one will be for all the files. We should have done it that way right from the very beginning...but hey...it is what it is...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1232a5f-932d-4967-9ede-163e6fcad340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing helper_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helper_file.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wrangle_bike_rentals import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def show_nulls(df):\n",
    "    return (df[df\n",
    "            .isna()\n",
    "            .any(axis=1)]\n",
    "           )\n",
    "    \n",
    "def total_nulls(df):\n",
    "    return (df\n",
    "         .isna()\n",
    "         .sum()\n",
    "         .sum()\n",
    "        )\n",
    "\n",
    "def get_data(url):\n",
    "    return (\n",
    "        pd.read_csv(url)\n",
    "    )\n",
    "\n",
    "\n",
    "url_census = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "\n",
    "def prep_census(url):\n",
    "    col_names = ['age', 'workclass', 'fnlwgt', \n",
    "                 'education', 'education-num', \n",
    "                 'marital-status', 'occupation',\n",
    "                  'relationship', 'race', 'sex', \n",
    "                 'capital-gain', 'capital-loss', \n",
    "                 'hours-per-week', 'native-country', \n",
    "                   'income']\n",
    "    return (pd\n",
    "            .read_csv(url, header=None)\n",
    "            .pipe(lambda x: x.rename(columns={i: name for i, name in enumerate(col_names)}))\n",
    "            .drop(['education'], axis=1)\n",
    "    )\n",
    "\n",
    "df_census = prep_census(url_census)\n",
    "\n",
    "def splitX_y(df, trgt):\n",
    "    features = [col for col in df.columns if col not in trgt]\n",
    "    return (df[features], df[trgt])\n",
    "\n",
    "def prep_bike_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())),\n",
    "                    hum = (data['hum']\n",
    "                   .fillna(data.groupby('season')['hum']\n",
    "                           .transform('median'))),\n",
    "                    dteday = pd.to_datetime(data['dteday']),\n",
    "                    mnth = lambda x: x['dteday'].dt.month,\n",
    "                    yr = data['yr'].ffill()\n",
    "                   )\n",
    "            .drop(['dteday', 'casual','registered'], axis=1)\n",
    "           )\n",
    "\n",
    "url_bikes = 'https://raw.githubusercontent.com/theAfricanQuant/XGBoost4machinelearning/main/data/bike_rentals.csv'\n",
    "\n",
    "class PrepDataTransformer(BaseEstimator,\n",
    "    TransformerMixin):\n",
    "    \"\"\"\n",
    "    This transformer takes a Pandas DataFrame containing our survey \n",
    "    data as input and returns a new version of the DataFrame. \n",
    "    \n",
    "    ----------\n",
    "    ycol : str, optional\n",
    "        The name of the column to be used as the target variable.\n",
    "        If not specified, the target variable will not be set.\n",
    "    Attributes\n",
    "    ----------\n",
    "    ycol : str\n",
    "        The name of the column to be used as the target variable.\n",
    "    \"\"\"\n",
    "    def __init__(self, ycol=None):\n",
    "        self.ycol = ycol\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return prep_bike_data(X)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "df_bikes = get_data(url_bikes)\n",
    "\n",
    "class OHETransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer that applies one-hot encoding to columns of type 'object' \n",
    "    in a Pandas DataFrame. The one-hot encoding process converts categorical \n",
    "    columns into a format that can be provided to machine learning algorithms \n",
    "    to improve predictions.\n",
    "\n",
    "    The transformer identifies columns with data type 'object' and uses the\n",
    "    OneHotEncoder from the category_encoders library to perform the encoding.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cols : list\n",
    "        List of column names in the DataFrame identified for one-hot encoding.\n",
    "    \n",
    "    encode : OneHotEncoder object\n",
    "        The encoder instance from category_encoders that performs the actual \n",
    "        one-hot encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cols = None\n",
    "        self.encode = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        self.encode = OneHotEncoder(cols=self.cols, use_cat_names=False)\n",
    "        self.encode.fit(X[self.cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_encoded = self.encode.transform(X[self.cols])\n",
    "        X = X.drop(columns=self.cols)\n",
    "        return pd.concat([X, X_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcb44a-278d-4f62-8f73-dedd85468aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
