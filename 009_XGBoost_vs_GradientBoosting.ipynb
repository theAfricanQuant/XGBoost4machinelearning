{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d3261c-832e-4b1c-9af5-ea2c19f1bc61",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">XGBoost vs Gradient Boosting</h1>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "XGBoost stands for \"Extreme Gradient Boosting,\" and it represents an advanced iteration of the gradient boosting technique. The fundamental idea behind XGBoost remains similar to its predecessor (that we tackled in the previous notebook) – it enhances the performance of weak learners by aggregating the errors or residuals of individual decision trees.\n",
    "\n",
    "However, in XGBoost, there is a notable terminology difference in the hyperparameters. Instead of referring to the learning rate as `learning_rate`, as it's commonly known in other gradient boosting implementations, XGBoost prefers to call it `eta`. This small naming variation aside, XGBoost harnesses the power of boosting to iteratively improve the predictive accuracy of a model, making it a robust tool in the world of machine learning and predictive analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f035276b-be91-44b2-ae4f-2ddec0c02be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import io\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from zipfile import ZipFile\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_file import *\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152e5254-7e63-4c40-88bb-c1c7b2b8986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>336</td>\n",
       "      <td>2011-12-02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.314167</td>\n",
       "      <td>0.331433</td>\n",
       "      <td>0.625833</td>\n",
       "      <td>0.100754</td>\n",
       "      <td>268</td>\n",
       "      <td>3672</td>\n",
       "      <td>3940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>632</td>\n",
       "      <td>2012-09-23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.529167</td>\n",
       "      <td>0.518933</td>\n",
       "      <td>0.467083</td>\n",
       "      <td>0.223258</td>\n",
       "      <td>2454</td>\n",
       "      <td>5453</td>\n",
       "      <td>7907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>621</td>\n",
       "      <td>2012-09-12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.599167</td>\n",
       "      <td>0.570075</td>\n",
       "      <td>0.577083</td>\n",
       "      <td>0.131846</td>\n",
       "      <td>1050</td>\n",
       "      <td>6820</td>\n",
       "      <td>7870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     instant      dteday  season   yr  mnth  holiday  weekday  workingday  \\\n",
       "335      336  2011-12-02     4.0  0.0  12.0      0.0      5.0         1.0   \n",
       "631      632  2012-09-23     4.0  1.0   9.0      0.0      0.0         0.0   \n",
       "620      621  2012-09-12     3.0  1.0   9.0      0.0      3.0         1.0   \n",
       "\n",
       "     weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "335           1  0.314167  0.331433  0.625833   0.100754     268        3672   \n",
       "631           1  0.529167  0.518933  0.467083   0.223258    2454        5453   \n",
       "620           1  0.599167  0.570075  0.577083   0.131846    1050        6820   \n",
       "\n",
       "      cnt  \n",
       "335  3940  \n",
       "631  7907  \n",
       "620  7870  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bikes.sample(n=3, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84285a01-b95d-4714-9c90-44f2076b00dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target vector: (731,)\n",
      "shape of feature matrix: (731, 15)\n"
     ]
    }
   ],
   "source": [
    "bikes_X, bikes_y = splitX_y(df_bikes, 'cnt')\n",
    "\n",
    "print(f\"shape of target vector: {bikes_y.shape}\")\n",
    "print(f\"shape of feature matrix: {bikes_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25d9ec1-3800-4cbf-8565-341ee8a537db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_X_train, bikes_X_test, bikes_y_train, bikes_y_test = train_test_split(\n",
    "        bikes_X, bikes_y, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f89177b-9f1f-450d-8f8f-197693695ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [('tweak', PrepDataTransformer()),\n",
    "     ('imputer', SimpleImputer(strategy='median')),  # Imputing null values using mean\n",
    "     ('scaler', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train = pipe.fit_transform(bikes_X_train)\n",
    "X_test = pipe.transform(bikes_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb4d00d-096c-4327-97eb-57f112a5497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'max_depth': 2, \n",
    "               'subsample': 0.5, \n",
    "               'n_estimators': 500, \n",
    "               'learning_rate': 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd4e22a-bc7e-443d-8747-d0a563b1d9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 598.540\n"
     ]
    }
   ],
   "source": [
    "best_model = GradientBoostingRegressor(**best_params, random_state=43)\n",
    "best_model.fit(X_train, bikes_y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse_test = MSE(bikes_y_test, y_pred)**0.5\n",
    "print(f\"Test set score: {rmse_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebeae81-9dad-43ce-bc84-a7132e4183c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'max_depth': 2, \n",
    "               'subsample': 0.5, \n",
    "               'n_estimators': 500, \n",
    "               'eta': 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32e0b38b-df0c-4c0c-b715-a70d930bc3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 604.429\n"
     ]
    }
   ],
   "source": [
    "xg_reg = XGBRegressor(**best_params, random_state=43)\n",
    "\n",
    "xg_reg.fit(X_train, bikes_y_train)\n",
    "\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(bikes_y_test, y_pred)**0.5\n",
    "print(f\"Test set score: {rmse_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150abee-191b-4ce9-b754-3c5081187524",
   "metadata": {},
   "source": [
    "These scores are not too different from one another. We might need to go deeper into `xgboost` using a different dataset.\n",
    "\n",
    "## Approaching big data – gradient boosting versus XGBoost\n",
    "\n",
    "This we will want to examine exoplanets over time. The exoplanet dataset contains 5,087 rows and 3,189 columns, recording light flux measurements at different times across a star's lifecycle. With 5,087 rows and 3,189 columns, there are a total of 1.5 million data points in the dataset.\n",
    "\n",
    "To build a random forest model on this dataset using a baseline of 100 trees, we would need 150 million data points. This is because a random forest creates each tree using a random subset of rows and columns from the full dataset. With 100 trees and 1.5 million data points, each tree would be trained on roughly 15,000 data points on average (1.5 million / 100). Thus, to properly train a 100-tree model, we need a dataset approximately 100 times larger, equating to 150 million data points.\n",
    "\n",
    "### Kepler's Starlight Dataset from Kaggle (circa 2017)\n",
    "\n",
    "Here's this cool dataset on [Kaggle](https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data). Now, here's the neat bit:\n",
    "\n",
    "- **What's Inside?** Patterns of starlight! Imagine every row as a unique star, and the columns? They're like chapters of a book, each telling a snippet of the star's luminous tale over time.\n",
    "- **Got Planets?** Oh, absolutely! There's a column named 'exoplanet'. A '2' in there? That star's got a buddy planet! A '1'? It's the star shining solo.\n",
    "\n",
    "And for those diving into the cosmic jargon, this dataset is all about the light these stars beam out. That's called **light flux** or, for the poetic ones among us, **luminous flux**. It's like the universe's brightness meter.\n",
    "\n",
    "The flux columns are floats, while the Label column is `2` for an exoplanet star and `1` for a non-exoplanet star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bafb54ad-1d5e-40c6-b438-6cf777ecaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"data/exoplanets.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee70c53-360d-42d5-a339-eedc65d313d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>FLUX.1</th>\n",
       "      <th>FLUX.2</th>\n",
       "      <th>FLUX.3</th>\n",
       "      <th>FLUX.4</th>\n",
       "      <th>FLUX.5</th>\n",
       "      <th>FLUX.6</th>\n",
       "      <th>FLUX.7</th>\n",
       "      <th>FLUX.8</th>\n",
       "      <th>FLUX.9</th>\n",
       "      <th>...</th>\n",
       "      <th>FLUX.3188</th>\n",
       "      <th>FLUX.3189</th>\n",
       "      <th>FLUX.3190</th>\n",
       "      <th>FLUX.3191</th>\n",
       "      <th>FLUX.3192</th>\n",
       "      <th>FLUX.3193</th>\n",
       "      <th>FLUX.3194</th>\n",
       "      <th>FLUX.3195</th>\n",
       "      <th>FLUX.3196</th>\n",
       "      <th>FLUX.3197</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>93.85</td>\n",
       "      <td>83.81</td>\n",
       "      <td>20.10</td>\n",
       "      <td>-26.98</td>\n",
       "      <td>-39.56</td>\n",
       "      <td>-124.71</td>\n",
       "      <td>-135.18</td>\n",
       "      <td>-96.27</td>\n",
       "      <td>-79.89</td>\n",
       "      <td>...</td>\n",
       "      <td>-78.07</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>-102.15</td>\n",
       "      <td>25.13</td>\n",
       "      <td>48.57</td>\n",
       "      <td>92.54</td>\n",
       "      <td>39.32</td>\n",
       "      <td>61.42</td>\n",
       "      <td>5.08</td>\n",
       "      <td>-39.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-38.88</td>\n",
       "      <td>-33.83</td>\n",
       "      <td>-58.54</td>\n",
       "      <td>-40.09</td>\n",
       "      <td>-79.31</td>\n",
       "      <td>-72.81</td>\n",
       "      <td>-86.55</td>\n",
       "      <td>-85.33</td>\n",
       "      <td>-83.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-32.21</td>\n",
       "      <td>-24.89</td>\n",
       "      <td>-4.86</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-11.70</td>\n",
       "      <td>6.46</td>\n",
       "      <td>16.00</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>532.64</td>\n",
       "      <td>535.92</td>\n",
       "      <td>513.73</td>\n",
       "      <td>496.92</td>\n",
       "      <td>456.45</td>\n",
       "      <td>466.00</td>\n",
       "      <td>464.50</td>\n",
       "      <td>486.39</td>\n",
       "      <td>436.56</td>\n",
       "      <td>...</td>\n",
       "      <td>-71.69</td>\n",
       "      <td>13.31</td>\n",
       "      <td>13.31</td>\n",
       "      <td>-29.89</td>\n",
       "      <td>-20.88</td>\n",
       "      <td>5.06</td>\n",
       "      <td>-11.80</td>\n",
       "      <td>-28.91</td>\n",
       "      <td>-70.02</td>\n",
       "      <td>-96.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>326.52</td>\n",
       "      <td>347.39</td>\n",
       "      <td>302.35</td>\n",
       "      <td>298.13</td>\n",
       "      <td>317.74</td>\n",
       "      <td>312.70</td>\n",
       "      <td>322.33</td>\n",
       "      <td>311.31</td>\n",
       "      <td>312.42</td>\n",
       "      <td>...</td>\n",
       "      <td>5.71</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>-3.73</td>\n",
       "      <td>30.05</td>\n",
       "      <td>20.03</td>\n",
       "      <td>-12.67</td>\n",
       "      <td>-8.77</td>\n",
       "      <td>-17.31</td>\n",
       "      <td>-17.35</td>\n",
       "      <td>13.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1107.21</td>\n",
       "      <td>-1112.59</td>\n",
       "      <td>-1118.95</td>\n",
       "      <td>-1095.10</td>\n",
       "      <td>-1057.55</td>\n",
       "      <td>-1034.48</td>\n",
       "      <td>-998.34</td>\n",
       "      <td>-1022.71</td>\n",
       "      <td>-989.57</td>\n",
       "      <td>...</td>\n",
       "      <td>-594.37</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-401.66</td>\n",
       "      <td>-357.24</td>\n",
       "      <td>-443.76</td>\n",
       "      <td>-438.54</td>\n",
       "      <td>-399.71</td>\n",
       "      <td>-384.65</td>\n",
       "      <td>-411.79</td>\n",
       "      <td>-510.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LABEL   FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6  FLUX.7  \\\n",
       "0      2    93.85    83.81    20.10   -26.98   -39.56  -124.71 -135.18   \n",
       "1      2   -38.88   -33.83   -58.54   -40.09   -79.31   -72.81  -86.55   \n",
       "2      2   532.64   535.92   513.73   496.92   456.45   466.00  464.50   \n",
       "3      2   326.52   347.39   302.35   298.13   317.74   312.70  322.33   \n",
       "4      2 -1107.21 -1112.59 -1118.95 -1095.10 -1057.55 -1034.48 -998.34   \n",
       "\n",
       "    FLUX.8  FLUX.9  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
       "0   -96.27  -79.89  ...     -78.07    -102.15    -102.15      25.13   \n",
       "1   -85.33  -83.97  ...      -3.28     -32.21     -32.21     -24.89   \n",
       "2   486.39  436.56  ...     -71.69      13.31      13.31     -29.89   \n",
       "3   311.31  312.42  ...       5.71      -3.73      -3.73      30.05   \n",
       "4 -1022.71 -989.57  ...    -594.37    -401.66    -401.66    -357.24   \n",
       "\n",
       "   FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
       "0      48.57      92.54      39.32      61.42       5.08     -39.54  \n",
       "1      -4.86       0.76     -11.70       6.46      16.00      19.93  \n",
       "2     -20.88       5.06     -11.80     -28.91     -70.02     -96.67  \n",
       "3      20.03     -12.67      -8.77     -17.31     -17.35      13.98  \n",
       "4    -443.76    -438.54    -399.71    -384.65    -411.79    -510.54  \n",
       "\n",
       "[5 rows x 3198 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with ZipFile(url, 'r') as z:\n",
    "    with z.open(z.namelist()[0]) as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dd99c73-82f8-47f0-bdf4-e46d585b57da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5087 entries, 0 to 5086\n",
      "Columns: 3198 entries, LABEL to FLUX.3197\n",
      "dtypes: float64(3197), int64(1)\n",
      "memory usage: 124.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c925b4-4aec-4d26-8eb5-01170dff1e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ccb89f9-2b42-4e9f-9bc5-064dcf2c61a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target vector: (5087,)\n",
      "shape of feature matrix: (5087, 3197)\n"
     ]
    }
   ],
   "source": [
    "X, y = splitX_y(df, 'LABEL')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=43)\n",
    "\n",
    "print(f\"shape of target vector: {y.shape}\")\n",
    "print(f\"shape of feature matrix: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a4e2a-7c63-4b6e-9b4d-b239ec957142",
   "metadata": {},
   "source": [
    "### Building gradient boosting classifiers\n",
    "\n",
    "It's time to build a gradient boosting classifier to predict whether stars host exoplanets. It should be noted that Gradient boosting classifiers work in the same manner as gradient boosting regressors. The difference is primarily in the scoring.\n",
    "\n",
    "Let's now compare `GradientBoostingClassifier` and `XGBoostClassifier` with the exoplanet dataset for its speed using the preceding code to mark time. We have set `max_depth=2` and `n_estimators=100` to limit the size of the model. Let's start with `GradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebbd6f97-ad96-4f7e-8100-9738d05570e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9889937106918238\n",
      "\n",
      "Run Time: 4.937528161207835 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "gbr = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=43)\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f'Score: {str(score)}')\n",
    "\n",
    "end = time()\n",
    "\n",
    "elapsed = end - start\n",
    "\n",
    "print(f'\\nRun Time: {elapsed/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19c36a30-a88c-4979-b4c9-6ca76b667e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9921383647798742\n",
      "\n",
      "Run Time: 19.283082485198975 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "y_train = y_train.map({1: 0, 2: 1})\n",
    "y_test = y_test.map({1: 0, 2: 1})\n",
    "\n",
    "xg_reg = XGBClassifier(n_estimators=100, max_depth=2, random_state=43)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f'Score: {str(score)}')\n",
    "\n",
    "end = time()\n",
    "\n",
    "elapsed = end - start\n",
    "\n",
    "print(f'\\nRun Time: {elapsed} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9f2f1-f501-453f-96dc-91813fcec0af",
   "metadata": {},
   "source": [
    "Wow! The difference in timing is just staggering. In the world of big data XGBoost is the bomb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
