{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7138dc0-81ec-4030-a704-a2344917e1a8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Data Wrangling</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18947abc-7bcf-441a-8ce7-03309ee5ec14",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Data wrangling**, often also referred to as **data munging**, is the process of cleaning, structuring, and enriching raw data into a desired format for better decision-making. It's a fundamental step in the data preparation process before analysis or processing. Data wrangling involves several tasks and can be quite complex depending on the state of the data and the desired outcome. \n",
    "\n",
    "Here are some steps you would follow:\n",
    "\n",
    "1. **Cleaning**: This involves removing, correcting, or handling corrupted, misformatted, incomplete, or inaccurate data. Common tasks include handling missing values, removing duplicates, and correcting data inconsistencies.\n",
    "2. **Transforming**: This involves converting data from one format or structure into another. Examples include normalizing and scaling, pivoting tables, or converting data types.\n",
    "3.  **Enriching**: Enhancing data with additional variables or attributes can make the dataset more useful. This could involve adding data from other sources or creating new derived variables.\n",
    "4.  **Validating**: Ensuring that the dataset meets certain criteria, often set by predefined rules or schemas. This can involve checking for data consistency, accuracy, and relevance.\n",
    "5.  **Structuring**: Organizing data in a way that is suitable for the intended purpose. This could involve grouping, sorting, or aggregating data, or reshaping datasets.\n",
    "6.  **Integrating**: Combining data from multiple sources, which may involve tasks like data alignment, deduplication, and dealing with data source discrepancies.\n",
    "\n",
    "## Chaining\n",
    "\n",
    "In this work, we will use a concept called _Chaining_. At its core, chaining is about executing multiple operations in a sequence, where the output of one operation feeds directly into the input of the next.\n",
    "\n",
    "Think of a factory assembly line where raw materials enter one end and go through several machines, each performing a specific task, before a finished product comes out the other end. At each stage, the output of one machine becomes the input for the next.\n",
    "\n",
    "In our case, using pandas, chaining mirrors this assembly line concept. Instead of machines, we have methods (functions associated with objects), and instead of raw material, we have data.\n",
    "\n",
    "One thing we will spend time doing would be to create a new column or alter an old one; and there are several ways we could do this. \n",
    "\n",
    "```python\n",
    "df['new_column'] = value_or_function\n",
    "\n",
    "df.assign(new_column=value_or_function)\n",
    "```\n",
    "\n",
    "Both of the methods above would give us a new column with the name stated or if the column already exists, to alter the contents based off of the values assigned to it. In this walk-through, we will strictly use the `.assign` method most of the time; as this lends itself to the method of chaining well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c7ef5-6a94-49b0-9bfe-a48491c99f76",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a8cc0-dc64-442d-9da6-45bb9fdef44f",
   "metadata": {},
   "source": [
    "## Datasets: Bike Rentals\n",
    "\n",
    "We are opting out for the bike rentals dataset from theÂ UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/index.php), a world-famous data warehouse that is free to the public. The author of Hands-On Gradient Boosting with XGBoost and scikit-learn adjusted from the original dataset. We will depend on this book for our walk-through here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff38f303-d90c-49f3-a3eb-5c10c446a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/theAfricanQuant/XGBoost4machinelearning/main/data/bike_rentals.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a9864f-66a6-4f55-b650-08cff742c909",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(url):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m         pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m      4\u001b[0m     )\n\u001b[1;32m----> 6\u001b[0m df_bikes \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(url):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m----> 3\u001b[0m         \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m      4\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def get_data(url):\n",
    "    return (\n",
    "        pd.read_csv(url)\n",
    "    )\n",
    "\n",
    "df_bikes = get_data(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85554fc2-d337-48f5-8975-80e428530436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f774527-1fcd-4566-895c-d55eb2fef5e8",
   "metadata": {},
   "source": [
    "## Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc2053-63f6-4735-8681-b1de9ab4a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bikes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16794b23-e4a9-4bb4-b1dc-769211132e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bikes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e7c55-e8db-4c71-bfd3-d8a02fcf3fab",
   "metadata": {},
   "source": [
    "**Correcting null values**\n",
    "\n",
    "The following code will sum the total number of null values in the dataset. We will chain the methods and functions together horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447734eb-50a6-44d1-a190-3e7d7823364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_nulls(df):\n",
    "    return (df\n",
    "         .isna()\n",
    "         .sum()\n",
    "         .sum()\n",
    "        )\n",
    "\n",
    "total_nulls(df_bikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a8a832-db23-448f-9e30-a41fc5176420",
   "metadata": {},
   "source": [
    "We will now create a function that can display the the rows that have null values. In creating a function, we plan to use it over and over again in this project or any other in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490addef-863c-4e9c-bf58-224664ce7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nulls(df):\n",
    "    return (df[df\n",
    "            .isna()\n",
    "            .any(axis=1)]\n",
    "           )\n",
    "\n",
    "show_nulls(df_bikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453df6bc-c36c-4aca-a9c1-05fab63e3dbc",
   "metadata": {},
   "source": [
    "We will replace the null values in the windspeed column with the median. We choose to use the median over the median because the mean tends to guarantee that half the data is greater than the given value and half the data is lower. The mean, by contrast, is vulnerable to outliers. This is the begining of building our wrangle function and we will call it `prep_data`. We will continue to build it step by step until the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19027a21-e027-4cd5-8d72-8b2137055867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())))           \n",
    "           )\n",
    "\n",
    "bikes = prep_data(df_bikes)\n",
    "show_nulls(bikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160b6e2-a716-44f3-b572-cbdc4a246a6b",
   "metadata": {},
   "source": [
    "For the 'windspeed' column, the null values may be replaced with the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c6f1c-61d5-428f-8971-f02a6549d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.iloc[[56, 81]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474b7a3-a6a1-4c8c-a31d-46ff41bead55",
   "metadata": {},
   "source": [
    "It's possible to get more nuanced when correcting null values by using a `groupby`.\n",
    "\n",
    "A `groupby` organizes rows by shared values. Since there are four shared seasons spread out among the rows, a `groupby` of seasons results in a total of four rows, one for each season. But each season comes from many different rows with different values. We need a way to combine, or aggregate, the values. Choices for the aggregate include `.sum()`, .`count()`, .`mean()`, and .`median()`. \n",
    "\n",
    "Grouping our dataframe by season with the `.median(numeric_only=True)` aggregate is achieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c742cba-07fc-4643-a89f-2f2c84bf9d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.groupby(['season']).median(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103da20-a9ea-4eaa-86d6-09d0eb968917",
   "metadata": {},
   "source": [
    "To correct the null values in the `'hum'` column (humidity), we can take the median humidity by season.\n",
    "```python\n",
    "bikes['hum'] = bikes['hum'].fillna()\n",
    "```\n",
    "The code that goes inside fillna is the desired values. The values obtained from groupby require the transform method as follows:\n",
    "\n",
    "```python\n",
    "bikes.groupby('season')['hum'].transform('median')\n",
    "```\n",
    "Bringing everything together:\n",
    "\n",
    "```python\n",
    "bikes['hum'] = (bikes['hum']\n",
    "                   .fillna(bikes.groupby('season')['hum']\n",
    "                           .transform('median'))\n",
    "                  )\n",
    "```\n",
    "\n",
    "However, to implement it we are going to use the method of chaining and the `.assign` method in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33538c3e-b5fd-4dbb-b856-f171243039b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())),\n",
    "                    hum = (data['hum']\n",
    "                   .fillna(data.groupby('season')['hum']\n",
    "                           .transform('median'))\n",
    "                  )\n",
    "                   )           \n",
    "           )\n",
    "\n",
    "bikes = prep_data(df_bikes)\n",
    "show_nulls(bikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb0527-3287-4634-9951-e6e7e2634e3f",
   "metadata": {},
   "source": [
    "from the above we can see that the column `'hum'` has been taken off the those with `'nan'` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcbc1e-9040-4f29-81c4-8e783c6326f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.iloc[[129, 213, 388]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508d70d-c240-405f-8199-2a52f56ac0bb",
   "metadata": {},
   "source": [
    "**Obtaining the median/mean from specific rows**\n",
    "\n",
    "In some cases, it may be advantageous to replace null values with data from specific rows.\n",
    "\n",
    "When correcting temperature, aside from consulting historical records, taking the mean temperature of the day before and the day after should give a good estimate. \n",
    "\n",
    "To find null values of the 'temp' column, enter the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773439a-11c7-4d13-8fd1-f68a24aa21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes[bikes['temp'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605b1fe-df49-444b-988c-23b6dce9b1c9",
   "metadata": {},
   "source": [
    "Index `701` contains null values.\n",
    "\n",
    "We will now find the mean temperature of the day before and the day after the 701 index, using the some steps:\n",
    "\n",
    "1. Let us sum the temperatures in rows `700` and `702` and divide by `2` for both the `'temp'` and `'atemp'` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67190c56-67c9-4b99-b5f6-46c02086222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vals(df, idx1, idx2, col):\n",
    "    return (\n",
    "        (df.iloc[idx1][col] + \n",
    "        df.iloc[idx2][col])/2\n",
    "    )\n",
    "\n",
    "mean_vals(bikes, 700, 702, 'atemp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982c45e-e772-4623-9792-ea1971f49e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())),\n",
    "                    hum = (data['hum']\n",
    "                   .fillna(data.groupby('season')['hum']\n",
    "                           .transform('median'))),\n",
    "                    temp = (data['temp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n",
    "                    atemp = (data['atemp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'atemp')))\n",
    "                   )           \n",
    "           )\n",
    "\n",
    "bikes = prep_data(df_bikes)\n",
    "show_nulls(bikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230762c9-6ac9-424f-a36d-dae8316153c0",
   "metadata": {},
   "source": [
    "**Extrapolate dates**\n",
    "\n",
    "`'dteday'` is meant to be a date column but the `.info` we ran earlier revealed to us that it was an object or a string. Date objects such as years and months must be extrapolated from `datetime` types. Lets convert the column to a `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1d548-f6d7-4176-ad27-bea9b12d2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())),\n",
    "                    hum = (data['hum']\n",
    "                   .fillna(data.groupby('season')['hum']\n",
    "                           .transform('median'))),\n",
    "                    temp = (data['temp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n",
    "                    atemp = (data['atemp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'atemp'))),\n",
    "                    dteday = pd.to_datetime(data['dteday'])\n",
    "                   )           \n",
    "           )\n",
    "\n",
    "bikes = (prep_data(df_bikes)\n",
    "        .info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f1eec-b3b1-4ced-9675-04f405417620",
   "metadata": {},
   "source": [
    "We have imported the `'datetime'` library above. \n",
    "\n",
    "We will convert the `'mnth'` column to the correct months extrpolated from the `'dteday'` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e25e6-2090-40f0-a77a-b01a2de1474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())),\n",
    "                    hum = (data['hum']\n",
    "                   .fillna(data.groupby('season')['hum']\n",
    "                           .transform('median'))),\n",
    "                    temp = (data['temp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n",
    "                    atemp = (data['atemp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'atemp'))),\n",
    "                    dteday = pd.to_datetime(data['dteday']),\n",
    "                    mnth = lambda x: x['dteday'].dt.month\n",
    "                   )           \n",
    "           )\n",
    "\n",
    "bikes = prep_data(df_bikes)\n",
    "show_nulls(bikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133100dc-5823-458e-8e4f-6173b0b2da9d",
   "metadata": {},
   "source": [
    "lets us check the last 5 values of the dataset we have worked on so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb70a9-f37b-465b-9fc5-a5f4a3de26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b9fdd-64bd-4b66-a733-7f3088cb34f3",
   "metadata": {},
   "source": [
    "We can see that even though the year value on the `'dteday'` column has 2012 all through, the value on the `'yr'` column is `1.0`. It probably means that the values have been normalized, probably between 0 & 1. I think this was done because normalized data is often more efficient due to the fact that machine learning weights do not have to adjust for different ranges.\n",
    "\n",
    "We will just use the forward fill for the null values here since the row with the null value is in the same month with the preceding row.\n",
    "\n",
    "```python\n",
    "data['yr'].ffill()\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```python\n",
    "data['yr'].fillna(method='ffill')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a77e75-a22d-4944-8660-df284a70a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())),\n",
    "                    hum = (data['hum']\n",
    "                   .fillna(data.groupby('season')['hum']\n",
    "                           .transform('median'))),\n",
    "                    temp = (data['temp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n",
    "                    atemp = (data['atemp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'atemp'))),\n",
    "                    dteday = pd.to_datetime(data['dteday']),\n",
    "                    mnth = lambda x: x['dteday'].dt.month,\n",
    "                    yr = data['yr'].ffill()\n",
    "                   )           \n",
    "           )\n",
    "\n",
    "bikes = prep_data(df_bikes)\n",
    "show_nulls(bikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709c367-bea7-47d2-aa2d-b1747d85108d",
   "metadata": {},
   "source": [
    "There are no more null values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8801c8-2822-49ff-8a9c-536f4ca0b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nulls(bikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45487fa1-be4d-4e6a-816c-d72e3afc28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c6737-1f82-470a-83e4-c6dacee38224",
   "metadata": {},
   "source": [
    "**Deleting non-numerical columns**\n",
    "\n",
    "For machine learning, all data columns should be numerical. According to `df.info()`, the only column that is not numerical is `'dteday'`. Furthermore, it's redundant since all date information exists in other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48b729-68c4-4f57-acb4-be7bf4bae517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())),\n",
    "                    hum = (data['hum']\n",
    "                   .fillna(data.groupby('season')['hum']\n",
    "                           .transform('median'))),\n",
    "                    temp = (data['temp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n",
    "                    atemp = (data['atemp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'atemp'))),\n",
    "                    dteday = pd.to_datetime(data['dteday']),\n",
    "                    mnth = lambda x: x['dteday'].dt.month,\n",
    "                    yr = data['yr'].ffill()\n",
    "                   )\n",
    "            .drop('dteday', axis=1)\n",
    "           )\n",
    "\n",
    "bikes = prep_data(df_bikes)\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5a600-d423-4565-9e5e-e461d49c037f",
   "metadata": {},
   "source": [
    "### Final Code\n",
    "\n",
    "The next thing we will do is to create a module with the functions that we have created so we could always call them whenever we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec96f9d-2483-4f00-bd0d-510689b1cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_wrangle.py\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def show_nulls(df):\n",
    "    return (df[df\n",
    "            .isna()\n",
    "            .any(axis=1)]\n",
    "           )\n",
    "    \n",
    "def total_nulls(df):\n",
    "    return (df\n",
    "         .isna()\n",
    "         .sum()\n",
    "         .sum()\n",
    "        )\n",
    "\n",
    "def get_data(url):\n",
    "    return (\n",
    "        pd.read_csv(url)\n",
    "    )\n",
    "\n",
    "def mean_vals(df, idx1, idx2, col):\n",
    "    return (\n",
    "        (df.iloc[idx1][col] + \n",
    "        df.iloc[idx2][col])/2\n",
    "    )\n",
    "\n",
    "def prep_data(data):\n",
    "    return (data\n",
    "            .assign(windspeed = data[\"windspeed\"]\n",
    "                    .fillna((data[\"windspeed\"]\n",
    "                             .median())),\n",
    "                    hum = (data['hum']\n",
    "                   .fillna(data.groupby('season')['hum']\n",
    "                           .transform('median'))),\n",
    "                    temp = (data['temp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'temp'))),\n",
    "                    atemp = (data['atemp']\n",
    "                            .fillna(mean_vals(data, 700, 702, 'atemp'))),\n",
    "                    dteday = pd.to_datetime(data['dteday']),\n",
    "                    mnth = lambda x: x['dteday'].dt.month,\n",
    "                    yr = data['yr'].ffill()\n",
    "                   )\n",
    "            .drop('dteday', axis=1)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede179f-5c16-442a-8533-0b041c3f9fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
