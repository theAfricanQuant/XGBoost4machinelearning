{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc1837f-9b1b-4937-b06c-a6bb1ad68cb1",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">XGBoost Alternative Base Learners</h1>\n",
    "\n",
    "### What are Base Learners?\n",
    "\n",
    "In the context of XGBoost, a base learner is an individual model that contributes to the overall prediction by focusing on specific aspects or patterns in the data. Think of it as a team member in a relay race, where each runner (base learner) has a specific section of the track (part of the problem) they're best at.\n",
    "\n",
    "XGBoost, being a boosting algorithm, works by combining multiple base learners to form a strong predictive model. Each new base learner tries to correct the errors made by the previous ones, leading to a more accurate and robust model over time.\n",
    "\n",
    "The term \"base learners\" in the context of ensemble learning methods like XGBoost refers to the individual models that constitute the building blocks of the final, more complex model. Let's break this down in a simple, Feynman-style explanation.\r\n",
    "\r\n",
    "#### Analogy: Building a House\r\n",
    "\r\n",
    "Imagine you're building a house. Instead of constructing it in one go, you start with smaller, foundational elements like bricks or wooden beams. Each of these elements is essential, but on their own, they're not very functional as a house. However, when you combine them strategically, they form the robust and complex structure of a house.\r\n",
    "\r\n",
    "### Base Learners in XGBoost\r\n",
    "\r\n",
    "In XGBoost:\r\n",
    "\r\n",
    "- **The \"House\"**: The final predictive model you're trying to build.\r\n",
    "- **The \"Bricks/Wooden Beams\"**: These are the base learners. Each base learner is a simple model (like a decision tree in the case of `gbtree`).\r\n",
    "\r\n",
    "###\n",
    "Base learners are called so because they are the basic, fundamental models that serve as the starting point in creating a more sophisticated and powerful ensemble model through methods like boosting in XGBoost. Each one contributes a part of the knowledge, and together, they form a comprehensive, strong predictive model.\n",
    " Why \"Base\"?\r\n",
    "\r\n",
    "- **Foundational Role**: Just like bricks in a house, each base learner forms a part of the foundation of the overall model. They're the starting point.\r\n",
    "- **Building Blocks**: They are the basic units that are combined and improved upon to create a more complex and powerful model.\r\n",
    "- **Simplicity**: Each base learner is usually a simple model. Their simplicity allows for specialization – each one learns and adapts to different parts or patterns in the data.\r\n",
    "- **Combined Strength**: Individually, these learners might be weak (like a single brick isn't much use in a storm), but when combined (like bricks in a house), they significantly improve the mode\r\n",
    "\r\n",
    "### Understanding \"Learners\"\r\n",
    "\r\n",
    "In the realm of machine learning, a \"learner\" is a model that learns from data. This learning process involves recognizing patterns, making predictions, or uncovering insights based on the input data it is trained on. The key attributes of learners in this context include:\r\n",
    "\r\n",
    "1. **Pattern Recognition**: Learners analyze data to identify patterns. For example, in a dataset of housing prices, a learner might discover that larger houses tend to be more expensive.\r\n",
    "\r\n",
    "2. **Adaptation**: They adapt their internal parameters based on the data they're exposed to. This is similar to how a student learns new topics in school and gets better with practice and study.\r\n",
    "\r\n",
    "3. **Predictive Ability**: Once trained, learners can make predictions on new, unseen data. If you've trained a learner on past housing data, it can estimate the price of a new house it's never seen before, based on its learned patterns.\r\n",
    "\r\n",
    "4. **Improvement Over Time**: As they are exposed to more data or more rounds of training, learners usually improve. Their predictions become more accurate, and they become better at handling complex data.\r\n",
    "\r\n",
    "### In the Context of XGBoost\r\n",
    "\r\n",
    "- **Individual Decision Makers**: Each base learner in XGBoost can be thought of as an individual decision-maker. It makes predictions based on the part of the data it has been trained on.\r\n",
    "\r\n",
    "- **Sequential Improvement**: In boosting methods like XGBoost, each subsequent learner pays more attention to the errors made by previous learners. This is a learning process where each step builds upon the previous one, refining and improving the overall predictions.\r\n",
    "\r\n",
    "- **Collective Knowledge**: While each learner has its own set of skills and knowledge, the real power lies in their combination. Together, they form a comprehensive model that leverages the collective learning from all individual learners.\r\n",
    "\r\n",
    "In summary, the \"learners\" in \"base learners\" are individual models that learn from data. They are the active components in the machine learning process, capable of understanding, adapting, and making predictions based on the data they're trained on. In ensemble methods like XGBoost, these learners work together, each contributing its piece of knowledge, to form a more accurate and robust predictive model.ng predictive model.\n",
    "\n",
    "### Types of Base Learners in XGBoost\n",
    "\n",
    "1. **gbtree**: This is the default and most commonly used base learner in XGBoost. It stands for Gradient Boosted Trees. Each gbtree is a decision tree that's optimized to correct the mistakes of the preceding trees in the series.\n",
    "\n",
    "2. **gblinear**: This base learner uses linear models instead of trees. It's like switching from a sprinter who's great at curves to a long-distance runner for a straight stretch in our relay analogy. It's particularly useful when the relationship in your data is more linear.\n",
    "\n",
    "3. **dart**: DART stands for Dropouts meet Multiple Additive Regression Trees. It's a variation of gbtree, with a twist: during training, it randomly drops (ignores) some of the trees. This is akin to occasionally skipping a runner to prevent over-reliance on any particular one, enhancing the model's ability to generalize and reducing overfitting.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "Now, let's look at the code examples provided in your file for each of these base learners. I'll retrieve the relevant code snippets.\n",
    "\n",
    "It appears that there are no direct code snippets associated with `gbtree`, `gblinear`, or `dart` in the provided content of the file. This could be due to the specific structure of the file or the way the content is organized.\n",
    "\n",
    "However, I can provide a general idea of how these base learners might be implemented in code using XGBoost in Python. Keep in mind that these are illustrative examples and may not exactly match the content of your file.\n",
    "\n",
    "### Illustrative Code Examples\n",
    "\n",
    "1. **gbtree**:\n",
    "   ```python\n",
    "   import xgboost as xgb\n",
    "\n",
    "   # parameters for gbtree\n",
    "   params = {\n",
    "       'objective': 'binary:logistic',\n",
    "       'booster': 'gbtree',\n",
    "       # other parameters as needed\n",
    "   }\n",
    "\n",
    "   # training the model\n",
    "   model = xgb.train(params, dtrain, num_boost_round=10)\n",
    "   ```\n",
    "\n",
    "2. **gblinear**:\n",
    "   ```python\n",
    "   import xgboost as xgb\n",
    "\n",
    "   # parameters for gblinear\n",
    "   params = {\n",
    "       'objective': 'reg:linear',\n",
    "       'booster': 'gblinear',\n",
    "       # other parameters as needed\n",
    "   }\n",
    "\n",
    "   # training the model\n",
    "   model = xgb.train(params, dtrain, num_boost_round=10)\n",
    "   ```\n",
    "\n",
    "3. **dart**:\n",
    "   ```python\n",
    "   import xgboost as xgb\n",
    "\n",
    "   # parameters for dart\n",
    "   params = {\n",
    "       'objective': 'reg:linear',\n",
    "       'booster': 'dart',\n",
    "       'sample_type': 'uniform',\n",
    "       'normalize_type': 'tree',\n",
    "       'rate_drop': 0.1,  # dropout rate\n",
    "       # other parameters as needed\n",
    "   }\n",
    "\n",
    "   # training the model\n",
    "   model = xgb.train(params, dtrain, num_boost_round=10)\n",
    "   ```\n",
    "\n",
    "In these examples, `dtrain` would be your training dataset, and `num_boost_round` specifies the number of boosting rounds. The parameters within `params` vary based on the base learner and the specific problem you're addressing.\n",
    "\n",
    "If you need more specific examples or explanations from your file, please let me know which parts to focus on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "980f0a70-422d-448c-bc73-54fd6624a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier, XGBRFRegressor, XGBRFClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import (LinearRegression, LogisticRegression, \n",
    "                                        Lasso, Ridge)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "from helper_file import *\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46ca502-5caa-4c73-b81f-f517dfca1658",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9cd0bb-58d0-4bf2-95e4-96a568b12a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, shuffle the data and use 5 splits with KFold using the following parameters\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=43)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc585354-5611-47c0-92ab-0e358bf683eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model(model):\n",
    "\n",
    "    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kfold)\n",
    "\n",
    "    rmse = (-scores)**0.5\n",
    "\n",
    "    return rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec13d98-d78e-466c-aa32-433a31f0e496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228.64606316805893"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(XGBRegressor(booster='gblinear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "136e2f3c-2f39-4298-a019-fd208a447c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.22870075899542"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6300d31-843d-4291-b71d-64ad96e7df1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.90246701513051"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(Lasso())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523ee3d4-fe51-4b8c-b840-73f36e62ceb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.67555683787269"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6592a43c-c4e1-4049-9432-639f207f876b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.73701409766304"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(XGBRegressor(booster='gbtree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8a892-8d9d-4578-99b8-b7f3283610cd",
   "metadata": {},
   "source": [
    "The `gbtree` performs worst than the `gblinear` meaning a linear model is best for this kind of problem. Infact, the simple linear regression did perform better than xgboost with the gbtree as a base learner.\n",
    "\n",
    "The hyperparameters for the `gblinear` booster in XGBoost are designed specifically for linear models, differing significantly from the `gbtree` booster's hyperparameters, which are more suitable for tree-based models. Let's break down these hyperparameters and explain some new terms:\r\n",
    "\r\n",
    "1. **reg_lambda**\r\n",
    "   - **Explanation**: It's a regularization term (L2, similar to Ridge Regression) that helps to prevent overfitting by penalizing large coefficients.\r\n",
    "   - **Default**: 0\r\n",
    "   - **Range**: [0, ∞)\r\n",
    "   - **Example**: Setting `reg_lambda` to a higher value (e.g., 1 or 2) can help reduce model complexity and overfitting.\r\n",
    "\r\n",
    "2. **reg_alpha**\r\n",
    "   - **Explanation**: This is another regularization term (L1, similar to Lasso Regression) used to prevent overfitting, particularly useful in situations with high dimensionality.\r\n",
    "   - **Default**: 0\r\n",
    "   - **Range**: [0, ∞)\r\n",
    "   - **Example**: Increasing `reg_alpha` (e.g., setting it to 0.5 or 1) can help in feature selection, as it tends to shrink the coefficients of less important features to zero.\r\n",
    "\r\n",
    "3. **updater**\r\n",
    "   - **Explanation**: This parameter dictates the algorithm used for optimizing the linear model in each boosting round.\r\n",
    "   - **Options**: `shotgun` (employs hogwild parallelism with coordinate descent for a non-deterministic solution) and `coord_descent` (ordinary coordinate descent for a deterministic solution).\r\n",
    "   - **Default**: `shotgun`\r\n",
    "   - **Note**: Coordinate descent optimizes the model by improving one parameter at a time while keeping others fixed.\r\n",
    "\r\n",
    "4. **feature_selector**\r\n",
    "   - **Explanation**: Determines the method for selecting features during the boosting process.\r\n",
    "   - **Options**:\r\n",
    "     - `cyclic`: Iterates over features one by one in a fixed order.\r\n",
    "     - `shuffle`: Similar to `cyclic`, but shuffles the features in each round.\r\n",
    "     - `random`: Chooses features randomly.\r\n",
    "     - `greedy`: Selects the feature with the highest gradient magnitude.\r\n",
    "     - `thrifty`: Approximately greedy, but reorders features based on the magnitude of their weight changes.\r\n",
    "   - **Default**: `cyclic`\r\n",
    "   - **Compatibility**: Needs to be paired with an appropriate `updater`. For `shotgun`, use `cyclic` or `shuffle`; for `coord_descent`, use `random`, `greedy`, or `thrifty`.\r\n",
    "   - **Note**: The `greedy` method can be computationally expensive, especially for large datasets.\r\n",
    "\r\n",
    "5. **top_k**\r\n",
    "   - **Explanation**: Specifies the number of features to be considered by the `greedy` and `thrifty` feature selectors during coordinate descent.\r\n",
    "   - **Default**: 0 (considers all features)\r\n",
    "   - **Range**: [0, maximum number of features]\r\n",
    "   - **Example**: Setting `top_k` to a value like 5 means that only the top 5 features, based on their importance, will be considered in each boosting round when using `greedy` or `thrifty`.\r\n",
    "\r\n",
    "In summary, these hyperparameters allow fine-tuning of the `gblinear` booster in XGBoost, offering control over regularization, feature selection, and the optimization algorithm. Adjusting these settings can significantly impact the model's performance, especially in terms of complexity, overfitting, and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0323c202-a1e9-4e2c-a2ca-e1cc6b8963d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  target  \n",
       "0 -0.002592  0.019907 -0.017646   151.0  \n",
       "1 -0.039493 -0.068332 -0.092204    75.0  \n",
       "2 -0.002592  0.002861 -0.025930   141.0  \n",
       "3  0.034309  0.022688 -0.009362   206.0  \n",
       "4 -0.002592 -0.031988 -0.046641   135.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes_data = load_diabetes()\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "# The independent variables (features)\n",
    "df_features = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "\n",
    "# The dependent variable (target)\n",
    "df_target = pd.DataFrame(diabetes_data.target, columns=[\"target\"])\n",
    "\n",
    "# Combining features and target into one DataFrame\n",
    "df_diabetes = pd.concat([df_features, df_target], axis=1)\n",
    "\n",
    "# To display the first few rows of the DataFrame\n",
    "display(df_diabetes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b41136-feb8-4039-b39c-6e88298aadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_grid_search(params, reg=XGBRegressor(booster='gblinear', objective='reg:squarederror')):\n",
    "\n",
    "    # Instantiate GridSearchCV as grid_reg\n",
    "    grid_reg = GridSearchCV(reg, params, scoring='neg_mean_squared_error', cv=kfold)\n",
    "    \n",
    "    # Fit grid_reg on X_train and y_train\n",
    "    grid_reg.fit(X, y)\n",
    "\n",
    "    # Extract best params\n",
    "    best_params = grid_reg.best_params_\n",
    "\n",
    "    # Print best params\n",
    "    print(f\"Best params: {best_params}\")\n",
    "    \n",
    "    # Compute best score\n",
    "    best_score = np.sqrt(-grid_reg.best_score_)\n",
    "\n",
    "    # Print best score\n",
    "    print(f\"Best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e97c93d-9ea8-4b0f-9f31-44cd106212b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'reg_alpha': 0.1}\n",
      "Best score: 55.18001653715114\n"
     ]
    }
   ],
   "source": [
    "reg_grid_search(params={'reg_alpha':[0.001, 0.01, 0.1, 0.5, 1, 5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a93b277-38a8-4973-b136-2f2b515753dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'reg_lambda': 0.001}\n",
      "Best score: 56.08981230197895\n"
     ]
    }
   ],
   "source": [
    "reg_grid_search(params={'reg_lambda':[0.001, 0.01, 0.1, 0.5, 1, 5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28667ce4-6618-444f-8bd9-86b851b9c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'feature_selector': 'shuffle'}\n",
      "Best score: 55.56479371486922\n"
     ]
    }
   ],
   "source": [
    "reg_grid_search(params={'feature_selector':['shuffle']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eefd5c9-329c-4432-9101-0dc49de5f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'feature_selector': 'greedy', 'updater': 'coord_descent'}\n",
      "Best score: 55.29281522007418\n"
     ]
    }
   ],
   "source": [
    "reg_grid_search(params={'feature_selector':['random', 'greedy', 'thrifty'], 'updater':['coord_descent'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "908c0f10-78de-490f-ae4a-be5999b10e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'feature_selector': 'greedy', 'top_k': 9, 'updater': 'coord_descent'}\n",
      "Best score: 55.29281522007418\n"
     ]
    }
   ],
   "source": [
    "reg_grid_search(params={'feature_selector':['greedy', 'thrifty'], 'updater':['coord_descent'], 'top_k':[3, 5, 7, 9]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae162020-9c93-4652-95d0-ced2db2a9cfd",
   "metadata": {},
   "source": [
    "### Linear datasets\n",
    "\n",
    "Let us create a linear dataset and show that it is truly linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23209122-e96a-4ac0-a500-523cea1a306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the range of X values from 1 to 100\n",
    "X = np.arange(1,100)\n",
    "\n",
    "#Declare a random seed using NumPy to ensure the consistency of the results\n",
    "np.random.seed(2) \n",
    "\n",
    "#Create an empty list defined as y\n",
    "y = []\n",
    "\n",
    "# Loop through X, multiplying each entry by a random number from -0.2 to 0.2\n",
    "for i in X:\n",
    "    y.append(i*np.random.uniform(-0.2, 0.2))\n",
    "\n",
    "# Transform y to a numpy array for machine learning\n",
    "y = np.array(y)\n",
    "\n",
    "# Reshape X and y so that they contain as many rows as members in the array \n",
    "# and one column since columns are expected as machine learning inputs with scikit-learn\n",
    "X = X.reshape(X.shape[0], 1)\n",
    "y = y.reshape(y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba6985df-920b-4178-9fc5-334a3075e158",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X and y are already defined\n",
    "plt.plot(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b5f9a-733e-4954-a587-55d5e151fd03",
   "metadata": {},
   "source": [
    "We now have a linear dataset that includes randomness in terms of X and y.\n",
    "\n",
    "Let's run the regression_model function again with gblinear as the base learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66a36dd7-48b0-46db-ad6b-c494006a9ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.3469143783045165"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(XGBRegressor(booster='gblinear', objective='reg:squarederror'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df04c4c2-1e96-408c-b4d6-c5724fa4567b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.275567742823819"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the regression_model function with gbtree as the base learner\n",
    "\n",
    "regression_model(XGBRegressor(booster='gbtree', objective='reg:squarederror'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ef187-1a73-4265-9999-f892b4bde8be",
   "metadata": {},
   "source": [
    "It is clear from the above that `gblinear` performs much better in our constructed linear dataset.\n",
    "\n",
    "We also try `LinearRegression` on the same dataset next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92eb578d-2f52-4e1b-8d05-3ecebacf7275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.346935807599345"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fe278-8096-46a1-b692-70c92d7e4e23",
   "metadata": {},
   "source": [
    "`gblinear` is a good choice when linear models might outperform tree-based ones. It slightly surpassed `LinearRegression` in real and simulated datasets. Ideal for large, linear datasets, `gblinear` also works for classification tasks, which we will soon explore.\n",
    "\n",
    "### Comparing dart\r\n",
    "The base learner` dar`t is similar to` gbtre`e in the sense that both are gradient boosted tree; they differ  primailr in that ` dar`t removes trees (called dropout) during each round of boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "615970ba-21aa-4c4a-8612-022fb09c6f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.73701334758088"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_diabetes(return_X_y=True)\n",
    "regression_model(XGBRegressor(booster='dart', objective='reg:squarederror'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1850acf-0765-4aec-a1c7-feeaced2e444",
   "metadata": {},
   "source": [
    "The `dart` base learner gives similar result as the `gbtree` base learner. The similarity of results is on account of the small dataset and the success of the `gbtree` default hyperparameters to prevent overfitting without requiring the dropout technique.\n",
    "\n",
    "Let's see how dart performs compared to gbtree on a larger dataset; we use classification this time around.\n",
    "\n",
    "### `dart` with XGBClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aebc98da-1805-4f83-ba00-533d26cbf547",
   "metadata": {},
   "source": [
    "# URL of the CSV file\n",
    "url = 'https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter06/heart_disease.csv'\n",
    "\n",
    "# Reading the CSV file directly from the URL\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Create a data folder if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# File path for saving\n",
    "file_path = 'data/census_cleaned.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d66cfb7-a22b-4c99-a135-1e8589a5c5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "census_df = pd.read_csv(file_path)\n",
    "census_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79b01b9c-9ed8-43ca-b44b-ff126264d249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target vector: (303,)\n",
      "shape of feature matrix: (303, 13)\n"
     ]
    }
   ],
   "source": [
    "census_X, census_y = splitX_y(census_df, 'target')\n",
    "\n",
    "print(f\"shape of target vector: {census_y.shape}\")\n",
    "print(f\"shape of feature matrix: {census_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86eb7988-7bd5-4aa7-b171-696d6d80d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(model):\n",
    "\n",
    "    scores = cross_val_score(model, census_X, census_y, scoring='accuracy', cv=kfold)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0cac2317-a427-45f7-ba3f-7ce9cbcd2890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795136612021858"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBClassifier(booster='gbtree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcf5ef5f-2442-4771-a959-436c4cf83af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795136612021858"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBClassifier(booster='dart'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72816b-ef75-4f03-86b5-bf37b3743582",
   "metadata": {},
   "source": [
    "Seeing how close to the outputs are, it's unclear whether trees have been dropped or the dropping of trees has had zero effect.\n",
    "\n",
    "We want to also try out `gblinear` on the data set. `gblinear` can work on classification problems by using the sigmoid function to scale the weights just like it does with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c04a9a83-6dfa-4987-b131-edd9520d356e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782295081967213"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBClassifier(booster='gblinear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f011c-f4c2-4e4c-a16e-081f67a3e9c2",
   "metadata": {},
   "source": [
    "Comparing that with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3af37bf-c75d-4144-a2ac-0d28c32a515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8281420765027322"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aaaa86-13fb-4717-ae1b-f8a33e8addc6",
   "metadata": {},
   "source": [
    "In this scenario, `gblinear` from XGBoost slightly underperforms logistic regression; it's important to note that for classification tasks, XGBoost's `gblinear` serves as a strong substitute to traditional logistic regression.\n",
    "\n",
    "### DART Hyperparameters Overview\r\n",
    "DART, an extension of the gbtree approach in XGBoost, introduces unique hyperparameters. These control the dropping of trees, influencing model behavior and performance..\r\n",
    "\r\n",
    "Key DART-specific Hyperparameters:\r\n",
    "\r\n",
    "1. **Sample Type (`sample_type`)**:\r\n",
    "   - **Purpose**: Dictates the method for tree dropout.\r\n",
    "   - **Options**: 'uniform' (equal chance for all trees) or 'weighted' (chance based on tree weights).\r\n",
    "   - **Default**: 'uniform'.\r\n",
    "   - **Range**: ['uniform', 'weighted'].\r\n",
    "\r\n",
    "2. **Normalize Type (`normalize_type`)**:\r\n",
    "   - **Purpose**: Determines how new tree weights are calculated relative to dropped trees.\r\n",
    "   - **Options**: 'tree' (equal to individual dropped tree weight) or 'forest' (equal to cumulative weight of dropped trees).\r\n",
    "   - **Default**: 'tree'.\r\n",
    "   - **Range**: ['tree', 'forest'].\r\n",
    "\r\n",
    "3. **Rate Drop (`rate_drop`)**:\r\n",
    "   - **Purpose**: Sets the proportion of trees to be dropped.\r\n",
    "   - **Default**: 0.0 (no dropout).\r\n",
    "   - **Range**: [0.0, 1.0].\r\n",
    "\r\n",
    "4. **One Drop (`one_drop`)**:\r\n",
    "   - **Purpose**: Guarantees at least one tree drop per round.\r\n",
    "   - **Default**: 0 (no guarantee).\r\n",
    "   - **Range**: [0, 1].\r\n",
    "\r\n",
    "5. **Skip Drop (`skip_drop`)**:\r\n",
    "   - **Purpose**: Adjusts the likelihood of skipping dropout entirely.\r\n",
    "   - **Default**: 0.0 (equal probability of dropping each tree).\r\n",
    "   - **Range**: [0.0, 1.0].\r\n",
    "\r\n",
    "Experimenting with these parameters allows for fine-tuning DART's behavior, potentially enhancing model scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "724dede2-8ac4-4910-aa02-ea2e13d348b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8049180327868852"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBClassifier(booster='dart', one_drop=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74296f5b-74ef-4d62-8fb7-2714b34ab3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.73701334758088"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', sample_type='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f594986d-9bbd-40fb-9dd4-a84a5b6fae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.73701334758088"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', normalize_type='forest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cfc0f689-a773-4b39-85c9-a77cbc753dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.211287303891616"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(XGBRegressor(booster='dart', objective='reg:squarederror', one_drop=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655b65e-98a0-49fa-8ca8-9a992e7796a1",
   "metadata": {},
   "source": [
    "When it comes to `rate_drop`, the percentage of trees that will be dropped, a range of percentages may be used with the `reg_grid_search` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "867ee2b1-54f9-4a2c-8252-8c98f945a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'rate_drop': 0.2}\n",
      "Best score: 61.924817223973925\n"
     ]
    }
   ],
   "source": [
    "reg_grid_search(params={'rate_drop':[0.01, 0.1, 0.2, 0.4]}, reg=XGBRegressor(booster='dart', objective='reg:squarederror', one_drop=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1386cf3-e4af-4347-8039-54ae8c8ef489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'skip_drop': 0.01}\n",
      "Best score: 64.79907448108045\n"
     ]
    }
   ],
   "source": [
    "reg_grid_search(params={'skip_drop':[0.01, 0.1, 0.2, 0.4]}, reg=XGBRegressor(booster='dart', objective='reg:squarederror'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998e884-6607-40e1-8943-50ee8a44e442",
   "metadata": {},
   "source": [
    "### Understanding DART in XGBoost:\n",
    "DART stands out as a notable choice in XGBoost. It takes all gbtree parameters, making it simple to switch from gbtree to DART while tweaking settings. Essentially, the benefit lies in experimenting with DART-specific parameters like one_drop, rate_drop, normalize, among others, to explore further improvements. Trying DART as a foundational component in XGBoost model development is certainly a worthwhile endeavor.\n",
    "\n",
    "With this grasp of DART, let's now shift our focus to random forests.\n",
    "\n",
    "### Exploring Random Forests in XGBoost\n",
    "In XGBoost, random forests can be implemented in two ways. One approach is to use random forests as the base learner. The other is to employ XGBoost's specialized versions: `XGBRFRegressor` for regression tasks and `XGBRFClassifier` for classification. We begin by considering random forests as alternative base learners within the XGBoost framework.\n",
    "\n",
    "In XGBoost, creating a random forest isn't as straightforward as setting a booster hyperparameter to 'random forest'. Instead, tweak the 'num_parallel_tree' hyperparameter. By default, it's 1, meaning each boosting round builds one tree. Increase this number, and you turn gbtree (or dart) into a boosted random forest. Here, each round constructs several trees in parallel, essentially forming a forest.\n",
    "\n",
    "Quick Overview of `num_parallel_tree`:\n",
    "- It specifies the tree count in each boosting round.\n",
    "- Default is 1.\n",
    "- Range is [1, infinity).\n",
    "- A value greater than 1 morphs the booster into a random forest.\n",
    "\n",
    "With multiple trees per round, the learner isn't just a single tree but a forest. As XGBoost shares hyperparameters with traditional random forests, setting 'num_parallel_tree' above 1 effectively creates a random forest base learner.\n",
    "\n",
    "Practical Experiment:\n",
    "- Use `XGBRegressor` with `booster=gbtree` and `num_parallel_tree=25`. This implies each boosting round has 25 trees.\n",
    "- The observed score was 65.96604877151103, almost identical to boosting a single gbtree. This similarity arises because gradient boosting learns from previous trees' mistakes, and a robust initial random forest leaves little room for improvement.\n",
    "\n",
    "Key Insight:\n",
    "Gradient boosting excels through its learning process. It's often more effective to use a smaller `num_parallel_tree` value, like 5.\n",
    "\n",
    "Experiment with `num_parallel_tree=5`:\n",
    "- The score marginally improved to 65.96445649315855, a subtle but real enhancement.\n",
    "\n",
    "Conclusion:\n",
    "In XGBoost, lower `num_parallel_tree` values often yield better random forests. With this understanding of random forest implementation in XGBoost, the next step is to explore building random forests as original XGBoost models.\n",
    "\n",
    "### Building Random Forests with XGBoost Models:\n",
    "XGBoost isn't just for gradient boosting; it also offers tools for creating random forests, namely XGBRFRegressor and XGBRFClassifier.\n",
    "\n",
    "As per the XGBoost documentation (check it out [here](https://xgboost.readthedocs.io/en/latest/tutorials/rf.html)), their random forest implementation is still experimental. This means default settings might change. Here's a snapshot of the defaults as of 2020:\n",
    "\n",
    "1. **Number of Estimators (`n_estimators`)**:\n",
    "   - In XGBRFRegressor and XGBRFClassifier, use 'n_estimators' instead of 'num_parallel_tree'.\n",
    "   - Default: 100 (translates to the number of parallel trees).\n",
    "   - Range: [1, infinity).\n",
    "\n",
    "2. **Learning Rate (`learning_rate`)**:\n",
    "   - Unlike gradient boosters, XGBRFRegressor and XGBRFClassifier generally don't benefit from adjusting the learning rate since they involve a single round of tree building.\n",
    "   - Default: 1.\n",
    "   - Range: [0, 1].\n",
    "\n",
    "3. **Subsampling Parameters (`subsample`, `colsample_by_node`)**:\n",
    "   - These are set lower than in Scikit-learn's random forest, reducing the chance of overfitting.\n",
    "   - Defaults: 0.8.\n",
    "   - Range: [0, 1].\n",
    "\n",
    "Remember, when using `XGBRFRegressor` and `XGBRFClassifier`, the method is more akin to bagging, as seen in traditional random forests, rather than gradient boosting. This distinction is key in understanding how these models function and should be configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50617a0c-e62d-4ae0-9ef9-3fab815999a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.82439992207376"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(XGBRFRegressor(objective='reg:squarederror'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6811b056-3e1a-4222-bb6f-f825dae53ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.80168283500812"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f560177-285b-4542-a249-9a77499276b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8118579234972676"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(XGBRFClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f922d21f-9dfa-4294-b3fe-7cf3408b7565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8117486338797815"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68e098-5be7-4a5b-9898-177be3470907",
   "metadata": {},
   "source": [
    "### Exploring XGBoost's Random Forest Capabilities:\n",
    "In XGBoost, setting `num_parallel_tree` above 1 turns your base learner into a random forest. However, remember, boosting thrives on learning from simpler models. Hence, keep `num_parallel_tree` close to 1 for most cases. Use random forests as base learners judiciously, particularly if single-tree boosting isn't cutting it.\n",
    "\n",
    "On the flip side, XGBoost's own random forest implementations, XGBRFRegressor and XGBRFClassifier, offer a solid alternative to scikit-learn's versions. They've shown comparable, if not better, performance. Considering XGBoost's reputation in the machine learning community, these tools are certainly worth integrating into your toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f17fe-22cc-456f-88b4-9ce41060e8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
